# PowerShell Scripting & Azure DevOps CI/CD
## A Complete Development Workflow Demo

---

## Slide 1: Title Slide
**PowerShell Scripting & Azure DevOps CI/CD**
*Building Quality Code with Automated Testing*

**Speaker Notes (2 minutes):**
- Welcome everyone! Today we'll explore a complete PowerShell development workflow
- We'll cover local development practices, automated testing, and CI/CD with Azure DevOps
- By the end, you'll understand how to build reliable, production-ready PowerShell scripts
- This is a hands-on demo - I'll be showing you real, working code
- Feel free to ask questions as we go, but I'll also have time at the end for Q&A

---

## Slide 2: Agenda
### What We'll Cover Today

1. **Introduction to Modern PowerShell Development** (5 min)
2. **Project Structure & Best Practices** (5 min)
3. **Writing Quality PowerShell Functions** (10 min)
4. **Local Development with Testing** (15 min)
5. **Azure DevOps CI/CD Pipeline** (15 min)
6. **Live Demo & Walkthrough** (8 min)
7. **Q&A** (2 min)

**Speaker Notes (3 minutes):**
- We'll start with why modern PowerShell development matters
- Then dive into the practical structure of a PowerShell project
- You'll see how to write functions with proper error handling and documentation
- We'll run tests locally using our controller script
- Then we'll see how the same tests run automatically in Azure DevOps
- Finally, a live demo bringing it all together
- This workflow can be applied to scripts of any size - from utilities to enterprise automation

---

## Slide 3: Why Modern PowerShell Development?
### The Old Way vs The New Way

**The Old Way:**
- âŒ Scripts written in isolation
- âŒ Manual testing ("it works on my machine")
- âŒ No version control
- âŒ Documentation in separate files (if at all)
- âŒ Production issues discovered by users

**The New Way:**
- âœ… Modular, testable functions
- âœ… Automated testing before deployment
- âœ… Version control with Git
- âœ… Self-documenting code
- âœ… Issues caught before production

**Speaker Notes (5 minutes):**
- Traditional PowerShell development often meant writing scripts and hoping they work
- Many admins test manually - run the script, see if it works, deploy
- This leads to "works on my machine" syndrome
- Modern development brings software engineering practices to scripting
- Key benefits: reliability, maintainability, collaboration
- Example: Imagine deploying a script to 1000 servers - you want to be confident it works
- With automated testing and CI/CD, you catch issues early
- This isn't just for developers - it's for anyone writing PowerShell professionally
- The time investment upfront saves hours of troubleshooting later

---

## Slide 4: Project Structure
### Organizing Your PowerShell Project

```
project-root/
â”œâ”€â”€ .azure-pipelines/
â”‚   â””â”€â”€ azure-pipelines.yml       # CI/CD configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ SomeFunction.ps1      # Your functions
â”‚   â””â”€â”€ controller.ps1            # Local dev controller
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ SomeFunction.Tests.ps1    # Pester tests
â””â”€â”€ TestResults/                   # Test output
```

**Key Principles:**
- Separate source code from tests
- Keep CI/CD config in version control
- Use a controller for local development

**Speaker Notes (5 minutes):**
- A good project structure makes everything easier
- Source code goes in src/ - this is what you'll deploy
- Tests go in tests/ - separate but parallel structure
- Azure pipeline config stays with the code - no surprise changes
- The controller.ps1 script is your local command center
- Why this structure? It scales from small scripts to large projects
- TestResults folder is gitignored - it's temporary output
- This structure mirrors professional software projects
- Makes it easy for new team members to understand the layout
- Pro tip: Add a README.md in each folder explaining its purpose

---

## Slide 5: PowerShell Best Practices
### Writing Quality Functions

**Essential Elements:**
1. **Comment-Based Help** - Built-in documentation
2. **Parameter Validation** - Catch errors early
3. **Error Handling** - Try/Catch blocks
4. **Proper Output** - Return objects, not strings
5. **Verb-Noun Naming** - Follow PowerShell conventions

**Example:**
```powershell
function Get-ProcessInfo {
    [CmdletBinding()]
    param(
        [Parameter(Mandatory=$false)]
        [string]$ProcessName
    )
    
    try {
        # Function logic here
    }
    catch {
        Write-Error "Error: $_"
        throw
    }
}
```

**Speaker Notes (5 minutes):**
- Let's talk about what makes a good PowerShell function
- Comment-based help: Users can run Get-Help on your function
- Include Synopsis, Description, Parameters, and Examples
- Parameter validation: Use attributes like [ValidateNotNull], [ValidateSet]
- This prevents bad input from reaching your logic
- Error handling: Always use try/catch for external calls
- Don't silently fail - log errors and throw when appropriate
- Proper output: Return rich objects, not formatted strings
- This allows piping and further processing
- Verb-Noun naming: Get-ProcessInfo, not GetProcessInfo or Get-Process-Info
- Use approved verbs: Get-Verb shows the full list
- These practices make your code professional and reusable

---

## Slide 6: Our Demo Functions
### What We're Testing Today

**Function 1: Get-ProcessInfo**
- Retrieves running process information
- Returns CPU usage and memory consumption
- Can filter by process name or get top N processes

**Function 2: Test-FileExists**
- Tests if a file or directory exists
- Returns boolean result
- Provides user-friendly colored output

**Why These Functions?**
- Simple enough to understand quickly
- Complex enough to demonstrate real testing
- Use common PowerShell cmdlets
- Representative of real-world automation tasks

**Speaker Notes (3 minutes):**
- For this demo, I've created two practical functions
- Get-ProcessInfo is useful for monitoring and reporting
- You might use this in health check scripts or performance monitoring
- Test-FileExists is a building block for larger scripts
- Many automation tasks start with "does this file exist?"
- These functions demonstrate key concepts without overwhelming complexity
- They use Get-Process and Test-Path - cmdlets you probably use daily
- The testing principles we'll show apply to any PowerShell code
- Whether you're working with Active Directory, Azure, or file systems
- The same approach works

---

## Slide 7: Understanding PSScriptAnalyzer
### Automated Code Quality

**What is PSScriptAnalyzer?**
- Static code analysis tool for PowerShell
- Catches common mistakes and anti-patterns
- Enforces best practices

**What It Checks:**
- Unused variables
- Missing comment-based help
- Improper verb usage
- Security issues (hardcoded credentials)
- Performance problems
- Compatibility issues

**Severity Levels:**
- **Error:** Must be fixed
- **Warning:** Should be fixed
- **Information:** Consider fixing

**Speaker Notes (5 minutes):**
- PSScriptAnalyzer is your first line of defense
- Think of it as a spell-checker for PowerShell code
- It runs instantly and catches obvious issues
- Example: Using Write-Host instead of Write-Output
- Example: Not using approved verbs in function names
- Example: Variables declared but never used
- It can catch security issues like plaintext passwords
- The tool has different rule sets - you can customize
- We run this locally AND in our pipeline
- Catching issues early saves time in code review
- It's like having an expert look over your shoulder
- Free tool, available in PowerShell Gallery
- Run it before committing code - make it a habit

---

## Slide 8: Introduction to Pester
### PowerShell Testing Framework

**What is Pester?**
- Unit testing framework for PowerShell
- Comes with PowerShell 5.0+ (update to v5+ recommended)
- Industry standard for PowerShell testing

**Key Concepts:**
- **Describe:** Groups related tests
- **Context:** Sub-groups within Describe
- **It:** Individual test cases
- **Should:** Assertions/expectations

**Test Structure:**
```powershell
Describe "FunctionName" {
    Context "When doing something" {
        It "Should behave correctly" {
            $result = Get-Something
            $result | Should -Be $expected
        }
    }
}
```

**Speaker Notes (5 minutes):**
- Pester is THE testing framework for PowerShell
- Developed by the PowerShell community, endorsed by Microsoft
- If you're testing PowerShell, you're using Pester
- The syntax is readable - almost like plain English
- Describe blocks group related tests together
- Context provides additional grouping and setup
- It blocks are individual tests - each one should test one thing
- Should is how you make assertions - "result should be X"
- Tests run in isolation - they don't affect each other
- You can mock functions for testing complex scenarios
- BeforeAll and AfterAll for setup and teardown
- Tests are documentation - they show how functions should behave
- Well-written tests are examples of usage

---

## Slide 9: Our Testing Strategy
### What We Test and Why

**Test Coverage:**

1. **Happy Path Tests**
   - Function returns expected data
   - Correct number of results
   - Proper object properties

2. **Edge Cases**
   - Empty results
   - Invalid input
   - Process/file doesn't exist

3. **Error Handling**
   - Exceptions thrown correctly
   - Error messages are clear

**Testing Philosophy:**
- Test behavior, not implementation
- Each test should be independent
- Tests should be fast
- Tests should be deterministic

**Speaker Notes (5 minutes):**
- Let's talk about what makes good tests
- Happy path: Test the normal, expected usage
- "When I ask for top 5 processes, I get 5 results"
- Edge cases: What happens at the boundaries?
- What if the process doesn't exist? What if the file is locked?
- Error handling: Verify errors occur when they should
- And that error messages help users understand what went wrong
- Test behavior, not implementation: Don't test internal variables
- Test what the function returns and how it behaves
- Independent tests: Each test can run alone or in any order
- Fast tests: You'll run them often - they need to be quick
- Deterministic: Same input always gives same output
- Our demo has 8 tests covering all these scenarios
- In production, you'd have more comprehensive coverage

---

## Slide 10: The Controller Script
### Local Development Command Center

**controller.ps1 Actions:**
```powershell
# Run linting only
.\src\controller.ps1 -Action lint

# Run tests only
.\src\controller.ps1 -Action test

# Run both lint and tests
.\src\controller.ps1 -Action all

# Clean test results
.\src\controller.ps1 -Action clean
```

**Benefits:**
- Consistent development experience
- Same commands for all developers
- Mirrors CI/CD pipeline locally
- Colored output for quick scanning
- Installs required modules automatically

**Speaker Notes (5 minutes):**
- The controller script is your local development hub
- Instead of remembering complex commands, use simple actions
- This gives everyone on the team the same experience
- New developer? Just run controller.ps1 -Action all
- It handles module installation automatically
- First run might install PSScriptAnalyzer and Pester
- The colored output makes it easy to spot issues
- Green means success, red means problems to fix
- This mirrors what happens in the pipeline
- If it passes locally, it should pass in Azure DevOps
- The controller is itself a good example of PowerShell scripting
- Uses advanced functions, parameter validation, error handling
- You can extend it: add a -Action deploy or -Action package
- Pro tip: Add this to your PowerShell profile for quick access

---

## Slide 11: Azure DevOps Pipeline Overview
### CI/CD Automation

**Pipeline Stages:**

1. **Build & Validate**
   - Install PSScriptAnalyzer
   - Run static code analysis
   - Fail if issues found

2. **Test**
   - Install Pester
   - Run all unit tests
   - Publish test results to Azure DevOps

3. **Package & Publish**
   - Create artifacts (main branch only)
   - Store scripts for deployment
   - Version and tag release

**Speaker Notes (5 minutes):**
- Now let's see how this runs automatically in Azure DevOps
- Every time you push code, the pipeline runs
- Stage 1 is Build & Validate - run linting
- Same PSScriptAnalyzer rules we ran locally
- If there are errors or warnings, the build fails
- Stage 2 is Test - run Pester tests
- All tests must pass to continue
- Test results are published - we'll see this in the UI
- Stage 3 is Package - only on main branch
- This creates artifacts ready for deployment
- Scripts are versioned and stored
- Each stage depends on the previous one succeeding
- This creates a quality gate - bad code doesn't progress
- The pipeline is defined in YAML - stored with your code
- No hidden configuration in the UI
- Changes to the pipeline are version controlled too

---

## Slide 12: Pipeline Triggers & Branches
### When Does the Pipeline Run?

**Trigger Configuration:**
```yaml
trigger:
  branches:
    include:
    - main
    - develop
  paths:
    include:
    - src/**
    - tests/**
    - .azure-pipelines/**
```

**Branch Strategy:**
- **develop:** Active development, all stages run
- **main:** Production-ready code, includes artifact publishing
- **feature branches:** Can be added to trigger

**Path Filtering:**
- Only runs when relevant files change
- Changes to docs don't trigger tests
- Saves build minutes

**Speaker Notes (5 minutes):**
- Triggers control when the pipeline runs automatically
- We trigger on main and develop branches
- Main is your production-ready code
- Develop is where active development happens
- You can add feature branches to the trigger list
- Path filtering is important for efficiency
- We only trigger when code changes (src/, tests/, or pipeline config)
- If someone updates README.md, no need to run tests
- This saves Azure DevOps build minutes
- Manual runs are always possible from the UI
- You can also trigger on pull requests
- This validates changes before merging
- Best practice: Require passing build before PR approval
- This prevents broken code from entering main
- You can set up branch policies in Azure DevOps
- Enforce code reviews plus passing tests

---

## Slide 13: Pipeline Job Details
### What Happens in Each Stage

**Stage 1: Build (Lint)**
```yaml
- task: PowerShell@2
  displayName: 'Run Script Analyzer'
  inputs:
    targetType: 'inline'
    script: |
      $scripts = Get-ChildItem -Path "src" -Filter *.ps1 -Recurse
      Invoke-ScriptAnalyzer -Path $script.FullName
```

**Stage 2: Test**
```yaml
- task: PowerShell@2
  displayName: 'Run Pester Tests'
  inputs:
    script: |
      Invoke-Pester -Configuration $config
      
- task: PublishTestResults@2
  inputs:
    testResultsFiles: '**/test-results.xml'
```

**Speaker Notes (5 minutes):**
- Let's look at the actual pipeline tasks
- PowerShell@2 is the Azure DevOps task for running PowerShell
- targetType 'inline' means script is in the YAML
- For linting, we get all .ps1 files and analyze them
- The script fails if issues are found (exit code 1)
- For testing, we use Pester's configuration object
- We specify where to save test results (XML format)
- PublishTestResults@2 uploads results to Azure DevOps
- This enables the nice UI with pass/fail charts
- You can track test results over time
- See which tests fail most often
- The 'condition: always()' means publish even if tests fail
- This way you see what failed in the UI
- Each task can have error handling and retry logic
- You can add notifications on failure
- Email, Teams, Slack - whatever your team uses

---

## Slide 14: Test Results in Azure DevOps
### Visibility and Tracking

**What You Get:**
- âœ… Pass/fail summary for each run
- âœ… Historical trending over time
- âœ… Detailed failure information
- âœ… Performance metrics
- âœ… Downloadable test result files

**Benefits:**
- Quick identification of breaking changes
- Team visibility into code quality
- Historical record for compliance
- Integration with work items

**Dashboard Example:**
```
Tests: 8 passed, 0 failed
Duration: 4.2 seconds
Coverage: 85%
Previous runs: âœ… âœ… âœ… âœ… âœ…
```

**Speaker Notes (5 minutes):**
- One of the best features: test result tracking in Azure DevOps
- After each run, you see a test summary
- How many passed, how many failed, how long it took
- Click into any test to see detailed output
- If a test fails, you see the exact assertion that failed
- Historical trending shows test reliability over time
- Is this test flaky? Does it fail intermittently?
- You can see when tests started failing
- Helps identify which commit broke what
- Great for teams: everyone sees the same results
- No more "it works on my machine"
- For compliance: tests provide auditable evidence
- You can link test results to work items
- "This bug fix is validated by these tests"
- Export results for reporting
- Set up quality gates: require X% pass rate
- The dashboard makes quality visible

---

## Slide 15: Artifacts and Deployment
### Packaging Your Scripts

**Artifact Creation:**
```yaml
- task: PublishBuildArtifacts@1
  inputs:
    PathtoPublish: '$(Build.ArtifactStagingDirectory)'
    ArtifactName: 'powershell-scripts'
```

**What Gets Published:**
- All scripts from src/
- Versioned with build number
- Stored in Azure DevOps
- Ready for deployment

**Deployment Options:**
- Manual download and deploy
- Release pipeline with approvals
- Automated deployment to environments
- Copy to file shares or package feeds

**Speaker Notes (5 minutes):**
- After tests pass, we create artifacts
- Artifacts are the packaged output of your build
- For PowerShell, this is your tested scripts
- They're stored in Azure DevOps
- Each artifact is versioned with the build number
- Build 1.0.47 has artifact 1.0.47
- You can download artifacts manually
- Or set up a release pipeline
- Release pipelines handle deployment
- They can have approval gates
- "Quality assurance must approve before production"
- You can deploy to multiple environments
- Dev, Test, Staging, Production
- Each with its own approval process
- Artifacts ensure you deploy exactly what was tested
- No "oops, forgot to copy that file"
- You can also publish to package feeds
- Make your scripts available via PowerShellGet
- For this demo, we keep it simple
- But the foundation for full deployment is there

---

## Slide 16: Live Demo - Part 1
### Local Development Workflow

**Demo Steps:**
1. Show project structure in VS Code
2. Open SomeFunction.ps1 - explain the code
3. Open SomeFunction.Tests.ps1 - explain tests
4. Run: `.\src\controller.ps1 -Action lint`
5. Run: `.\src\controller.ps1 -Action test`
6. Show TestResults folder
7. Introduce an intentional error
8. Show how lint/tests catch it

**Speaker Notes (8 minutes):**
- Now for the exciting part - let's see this in action!
- First, I'll open the project in VS Code
- See the folder structure we discussed
- Let me open SomeFunction.ps1
- Here's Get-ProcessInfo - notice the comment-based help
- Parameters with validation
- Try/catch error handling
- It returns a custom object with specific properties
- Now the tests - SomeFunction.Tests.ps1
- BeforeAll block loads the function
- Describe blocks group tests
- Each It block tests one specific behavior
- Let's run the linter: controller.ps1 -Action lint
- [Run command, show output]
- Green means all clear - no issues found
- Now let's run tests: controller.ps1 -Action test
- [Run command, show output]
- 8 tests, all passed, took about 4 seconds
- Let me show the TestResults folder
- There's our NUnit XML file
- Now let me introduce an error intentionally
- [Make a coding mistake, like missing a bracket]
- Run lint again - see? It caught it!
- This is how you catch issues before committing

---

## Slide 17: Live Demo - Part 2
### Azure DevOps Pipeline

**Demo Steps:**
1. Open Azure DevOps in browser
2. Navigate to Pipelines
3. Show a recent successful run
4. Walk through each stage
5. Show test results tab
6. Show artifacts
7. Demonstrate how to download

**Alternative if no live Azure DevOps:**
- Show screenshots of pipeline runs
- Explain what each section means
- Discuss how to set up a new pipeline

**Speaker Notes (7 minutes):**
- Now let's see this running in Azure DevOps
- [Open Azure DevOps]
- Here's our project, click on Pipelines
- We have several runs here - all successful
- Let me open the most recent one
- See the three stages: Build, Test, Package
- Each stage has a green checkmark - all passed
- Click on the Build stage
- Here's the lint job - it found all our scripts
- Analyzed them with PSScriptAnalyzer
- Zero issues found - success!
- Now the Test stage
- Installed Pester, ran tests
- Let's look at the test results tab
- Beautiful summary: 8 tests passed
- Click on any test to see details
- You can see the actual Pester output
- Now the Package stage
- This created our artifact
- Click on "1 published" - here's our artifact
- Named "powershell-scripts"
- You can download it right from here
- [Download if possible]
- Unzip it - there's our tested scripts
- Ready to deploy anywhere
- This is the power of automation

---

## Slide 18: Best Practices Summary
### Key Takeaways

**Code Quality:**
- âœ… Write modular, reusable functions
- âœ… Include comment-based help
- âœ… Use proper error handling
- âœ… Follow PowerShell naming conventions

**Testing:**
- âœ… Test locally before committing
- âœ… Aim for comprehensive coverage
- âœ… Keep tests fast and independent
- âœ… Use meaningful test names

**CI/CD:**
- âœ… Automate everything
- âœ… Fail fast with linting
- âœ… Track test results over time
- âœ… Version your artifacts

**Speaker Notes (3 minutes):**
- Let's recap the key best practices
- Start with quality code - modular functions
- Documentation is code - write it as you go
- Don't skip error handling - it will bite you
- Testing locally saves time - catch issues early
- Comprehensive coverage means testing happy paths AND errors
- Fast tests mean you'll actually run them
- Automation removes human error from the equation
- Linting catches issues immediately
- Test tracking shows quality trends
- Versioned artifacts ensure repeatability
- These practices scale from small scripts to large projects
- Start small - add one test, then another
- Build the habit of testing
- Your future self will thank you

---

## Slide 19: Getting Started in Your Environment
### Implementation Roadmap

**Week 1: Foundation**
- Install PSScriptAnalyzer and Pester
- Create basic project structure
- Write your first test

**Week 2: Local Development**
- Create a controller script
- Add linting to your workflow
- Write tests for existing scripts

**Week 3: CI/CD Setup**
- Set up Azure DevOps project
- Create your first pipeline
- Connect to your repository

**Week 4: Refinement**
- Add more comprehensive tests
- Configure branch policies
- Set up notifications

**Speaker Notes (3 minutes):**
- You might be thinking "this looks great, but where do I start?"
- Here's a practical roadmap
- Week 1: Just get the tools installed
- Install-Module PSScriptAnalyzer, Pester
- Create the folder structure
- Write one simple test for one function
- Week 2: Make it part of your workflow
- Build that controller script we showed
- Start running lint before you commit
- Add one test for each new function you write
- Week 3: Move to the cloud
- Set up an Azure DevOps organization (it's free for small teams)
- Create your first pipeline - copy our YAML
- Connect it to your Git repository
- Week 4: Polish and improve
- Add edge case tests
- Set up pull request policies
- Configure notifications so the team knows about failures
- Don't try to do everything at once
- Incremental improvement is the key

---

## Slide 20: Common Challenges & Solutions
### Troubleshooting Tips

**Challenge 1: Tests are slow**
- Solution: Mock external calls, use -Fast parameter
- Don't test external services in unit tests

**Challenge 2: Tests fail inconsistently**
- Solution: Remove dependencies on external state
- Use $TestDrive for file operations

**Challenge 3: Hard to test legacy code**
- Solution: Refactor into testable functions
- Start with new code, gradually improve old

**Challenge 4: Pipeline fails but works locally**
- Solution: Check PowerShell version, module versions
- Use same environment locally and in pipeline

**Speaker Notes (3 minutes):**
- Let's address common issues you might face
- Slow tests: Often caused by calling actual APIs or services
- Use Pester's mocking to fake external calls
- Unit tests should be fast - seconds, not minutes
- Flaky tests are a nightmare
- Usually caused by depending on machine state
- "Test only passes if file X exists on C drive"
- Use Pester's $TestDrive for temporary files
- Legacy code is hard to test
- Don't try to test it all at once
- Refactor gradually - extract testable functions
- Add tests as you modify code
- "Works on my machine" strikes again in pipelines
- Check PowerShell versions match
- Check module versions match
- Best practice: specify exact versions in pipeline
- Use containers for consistent environments

---

## Slide 21: Advanced Topics
### Where to Go from Here

**Testing:**
- Code coverage reporting
- Integration testing
- Performance testing
- Mocking complex scenarios

**CI/CD:**
- Multi-stage deployments
- Approval gates
- Rollback strategies
- Infrastructure as Code

**Tooling:**
- Visual Studio Code extensions
- GitHub Actions (alternative to Azure DevOps)
- PSake for build automation
- Plaster for project templates

**Speaker Notes (3 minutes):**
- Once you master the basics, there's more to explore
- Code coverage: What percentage of your code has tests?
- Tools like Pester can generate coverage reports
- Integration testing: Test how components work together
- Performance testing: Ensure scripts run efficiently
- Advanced mocking: Simulate complex external dependencies
- Multi-stage deployments: Dev -> Test -> Staging -> Prod
- Approval gates: Require sign-off before production
- Rollback: Plan for when deployments go wrong
- Infrastructure as Code: Test your infrastructure scripts too
- VS Code has great extensions for PowerShell and Pester
- Run tests right in the editor
- GitHub Actions is an alternative to Azure DevOps
- Free for public repositories
- PSake is a build automation tool for PowerShell
- Like Make but for PowerShell
- Plaster creates project templates
- Scaffold new projects with best practices built-in

---

## Slide 22: Resources & Documentation
### Learning More

**Official Documentation:**
- docs.microsoft.com/powershell
- pester.dev
- azure.com/devops/pipelines

**Community Resources:**
- PowerShell.org forums
- Reddit: r/PowerShell
- PowerShell Summit videos

**Books:**
- "The Pester Book" - free online
- "PowerShell Practice and Style Guide"
- "The DevOps Handbook"

**Tools:**
- PSScriptAnalyzer: github.com/PowerShell/PSScriptAnalyzer
- Pester: pester.dev
- VS Code PowerShell Extension

**Speaker Notes (2 minutes):**
- Here are resources to continue learning
- Microsoft's docs are comprehensive and well-maintained
- Pester.dev has excellent documentation and examples
- Azure DevOps docs cover pipeline scenarios
- PowerShell.org is a great community
- Active forums, helpful members
- Reddit's PowerShell community is also very active
- PowerShell Summit talks are on YouTube
- Hours of advanced content from experts
- "The Pester Book" is free and comprehensive
- Written by the Pester maintainers
- PowerShell Practice and Style Guide is community-driven
- Shows you the accepted way to write PowerShell
- The DevOps Handbook explains the philosophy
- Not PowerShell-specific but highly relevant
- All the tools we used today are open source
- Active communities, regular updates

---

## Slide 23: Q&A
### Questions?

**Common Questions:**
- Q: Do I need Azure DevOps or can I use GitHub?
  - A: GitHub Actions works too! Similar YAML syntax

- Q: What if my company doesn't use Git?
  - A: Azure DevOps supports TFVC too

- Q: How much does this cost?
  - A: Azure DevOps is free for small teams (up to 5 users)

- Q: Can I test GUI scripts?
  - A: Pester can test PowerShell with UI, but separate logic from UI

- Q: What about Windows PowerShell vs PowerShell 7?
  - A: Both work! Specify in your pipeline which to use

**Speaker Notes (5 minutes remaining):**
- Now I'd like to open it up for questions
- [Wait for questions, use these as prompts if none]
- [For each question, provide detailed answer]
- GitHub Actions: Yes, absolutely! The workflow is similar
- The tests and linting are the same
- Just different YAML syntax for the pipeline
- TFVC: Less common now, but Azure DevOps supports it
- Git is recommended for modern workflows
- Cost: Azure DevOps is free for small teams
- Free tier includes pipelines, repos, boards
- Paid only if you need more parallel jobs
- GUI testing: Separate your logic from UI
- Test the business logic with Pester
- UI testing needs different tools (like Pester for UI)
- PowerShell version: Both work great
- Pipeline can use either version
- pwsh: true for PowerShell 7
- pwsh: false for Windows PowerShell
- [Answer any other questions that come up]
- Thank you all for your time!

---

## Slide 24: Thank You!
### Contact & Follow-Up

**Demo Resources:**
- GitHub Repository: [Your repo URL]
- Sample Code: Available in the demo folder
- Presentation Slides: [URL]

**Stay Connected:**
- Email: [Your email]
- LinkedIn: [Your profile]
- Blog/Website: [Your site]

**Next Steps:**
1. Clone the demo repository
2. Try running it locally
3. Set up your own pipeline
4. Share your experience with the team!

**Thank you for attending!**

**Speaker Notes (2 minutes):**
- Thank you all for your attention today
- I hope this was helpful and practical
- All the code we used is available
- [Share repository link]
- Feel free to clone it and experiment
- The slides will be shared after this session
- If you have follow-up questions, reach out
- [Provide contact information]
- I encourage you to try this on a small project
- Start simple, build the habit
- Share what you learn with your team
- Good testing practices spread through example
- Remember: The goal isn't perfection
- It's continuous improvement
- Each test you write makes your code better
- Each pipeline run increases confidence
- Thank you again, and happy scripting!

---

## Appendix: Additional Slides (If Needed)

### Backup Slide 1: Pester Syntax Deep Dive
[Include if audience wants more Pester details]

### Backup Slide 2: Azure DevOps Pricing
[Include if cost questions come up]

### Backup Slide 3: Git Branching Strategy
[Include if discussion goes into version control]

### Backup Slide 4: Security Considerations
[Include for enterprise audience]

---

## Backup Slide 1: Pester Syntax Deep Dive
### Understanding Pester Commands

**Core Assertions:**
```powershell
# Equality
$result | Should -Be 5
$result | Should -Not -Be 0

# Null checks
$result | Should -BeNullOrEmpty
$result | Should -Not -BeNullOrEmpty

# Type checking
$result | Should -BeOfType [string]
$result | Should -BeOfType [System.Collections.ArrayList]

# Existence
$result | Should -Exist  # For files
$result | Should -Contain "value"  # For arrays

# Exceptions
{ Get-Something } | Should -Throw
{ Get-Something } | Should -Not -Throw

# Matching
$result | Should -Match "pattern"
$result | Should -BeLike "wild*card"
```

**Lifecycle Hooks:**
```powershell
BeforeAll { }      # Runs once before all tests
BeforeEach { }     # Runs before each test
AfterEach { }      # Runs after each test
AfterAll { }       # Runs once after all tests
```

**Mocking:**
```powershell
Mock Get-Process { return @{Name="fake"} }
Assert-MockCalled Get-Process -Times 1
```

**Speaker Notes:**
- These are advanced Pester features
- Should -Be is for exact equality
- Should -BeOfType verifies object types
- Useful when functions should return specific types
- Should -Throw tests exception handling
- Critical for testing error scenarios
- Mocking replaces real function calls with fake data
- Useful for testing without external dependencies
- Example: Mock Get-ADUser to test without Active Directory
- BeforeAll/AfterAll for expensive setup
- Like creating test databases or files
- BeforeEach for test isolation
- Each test gets fresh state
- Mock assertions verify function was called
- Great for testing that logging happened
- These advanced features unlock complex testing scenarios

---

## Backup Slide 2: Azure DevOps Pricing & Plans
### Understanding Costs

**Free Tier Includes:**
- âœ… Unlimited private Git repositories
- âœ… Up to 5 users
- âœ… 1 Microsoft-hosted parallel job (1,800 min/month)
- âœ… 1 self-hosted parallel job
- âœ… Azure Boards and Test Plans (basic)

**Paid Options:**
- **Basic Plan:** $6/user/month
  - Everything in free tier
  - More users
- **Basic + Test Plans:** $52/user/month
  - Advanced testing features
  - Manual testing management

**Build Minutes:**
- Free: 1,800 minutes/month
- Additional: $40/month per parallel job
- Self-hosted agents: Free, unlimited

**When to Upgrade:**
- Team has more than 5 people
- Need more parallel jobs for faster builds
- Require advanced test management
- Need more build minutes

**Alternatives:**
- GitHub Actions (Free for public repos, 2,000 min/month for private)
- GitLab CI/CD (Free tier available)
- Jenkins (Free, self-hosted)

**Speaker Notes:**
- Let's talk about the financial aspect
- Azure DevOps is very generous with free tier
- 5 users is enough for small teams
- 1,800 minutes is about 60 minutes per day
- For PowerShell scripts, builds are fast
- You'll rarely hit the limit
- Self-hosted agents are completely free
- Run on your own server
- Unlimited minutes, unlimited parallel jobs
- Only limitation: you manage the infrastructure
- Basic plan at $6/user is reasonable
- Compare to coffee subscriptions
- GitHub Actions is a solid alternative
- 2,000 minutes free for private repos
- Unlimited for public open-source projects
- GitLab has competitive free tier
- Jenkins is free but requires setup
- Most small teams stick with free tier
- Upgrade only when you have clear need
- ROI calculation: How much time does automation save?
- Usually pays for itself quickly

---

## Backup Slide 3: Git Branching Strategies
### Managing Your Code Flow

**Common Strategies:**

**1. GitHub Flow (Simple)**
```
main (always deployable)
  â””â”€â”€ feature/add-logging
  â””â”€â”€ feature/fix-bug-123
```
- Create feature branch
- Make changes, push
- Open pull request
- Merge to main after review

**2. GitFlow (Complex)**
```
main (production)
  â””â”€â”€ develop (integration)
      â””â”€â”€ feature/new-feature
      â””â”€â”€ hotfix/critical-bug
      â””â”€â”€ release/v1.0
```

**3. Trunk-Based (Fast)**
```
main (continuous integration)
  â””â”€â”€ Short-lived branches (<1 day)
```

**Recommended for PowerShell Projects:**
- Small teams: GitHub Flow
- Enterprise: GitFlow
- Continuous deployment: Trunk-Based

**Pull Request Best Practices:**
- Require code review
- Require passing tests
- Require up-to-date branches
- Use templates for consistency

**Speaker Notes:**
- Let's talk about managing code with branches
- Three main strategies, each has trade-offs
- GitHub Flow is simplest
- Perfect for small teams and simple projects
- Main branch is always production-ready
- Feature branches for new work
- Merge via pull requests
- GitFlow is more complex
- Has develop branch for integration
- Release branches for preparing releases
- Hotfix branches for urgent production fixes
- Good for scheduled releases
- Common in enterprise environments
- Trunk-Based is fastest
- Everyone commits to main frequently
- Short-lived branches, less than a day
- Requires mature testing and CI/CD
- For PowerShell projects, I recommend GitHub Flow
- Simple, effective, easy to understand
- Pull requests enable review and testing
- Set up branch policies in Azure DevOps
- Require at least one reviewer
- Require build validation (our pipeline)
- Require branch up-to-date before merge
- This prevents conflicts and broken code
- Use PR templates to standardize
- Checklist: "Did you update tests?"
- Good branching strategy prevents chaos
- Everyone knows where code should go

---

## Backup Slide 4: Security Considerations
### Keeping Your Scripts Safe

**Secrets Management:**
- âŒ Never hardcode passwords in scripts
- âŒ Never commit credentials to Git
- âœ… Use Azure Key Vault
- âœ… Use pipeline variables with encryption
- âœ… Use managed identities when possible

**Code Security:**
```powershell
# Bad - hardcoded credential
$password = "MyP@ssw0rd"

# Good - from Key Vault
$password = Get-AzKeyVaultSecret -VaultName "myvault" -Name "password"

# Better - managed identity
Connect-AzAccount -Identity
```

**Pipeline Security:**
- Use variable groups for shared secrets
- Mark variables as secret
- Limit pipeline access to specific branches
- Use service connections for external services
- Enable audit logging

**Code Review for Security:**
- Check for hardcoded secrets
- Verify input validation
- Look for SQL injection risks
- Review file path handling
- Check for command injection

**PSScriptAnalyzer Security Rules:**
- AvoidUsingPlainTextForPassword
- AvoidUsingConvertToSecureStringWithPlainText
- AvoidUsingUsernameAndPasswordParams
- UsePSCredentialType

**Speaker Notes:**
- Security is critical, especially in enterprise
- Number one mistake: hardcoding credentials
- "It's just a test script" becomes production
- Git history remembers everything
- Even if you delete it later, it's still there
- Use Azure Key Vault for secrets
- Secure storage, access logging, rotation
- Pipeline variables can be encrypted
- Marked as secret, never displayed in logs
- Managed identities are best for Azure
- No credentials to manage at all
- Service connections for external services
- GitHub, Docker Hub, AWS, etc.
- Credentials stored once, referenced by name
- Variable groups for shared secrets
- One place to update, used by multiple pipelines
- Code review catches security issues
- Check for hardcoded passwords, API keys
- Look for insecure patterns
- PSScriptAnalyzer has security rules
- Catches common mistakes automatically
- AvoidUsingPlainTextForPassword
- This would flag our bad example
- UsePSCredentialType
- Ensures proper credential handling
- Input validation prevents injection attacks
- Especially with user-provided data
- Never trust input, always validate
- Security is everyone's responsibility
- Build it into your process from day one

---

## Backup Slide 5: Performance Testing PowerShell
### Measuring Script Efficiency

**Measure-Command:**
```powershell
$time = Measure-Command {
    Get-ProcessInfo -Top 1000
}
Write-Host "Execution time: $($time.TotalSeconds) seconds"
```

**Pester Performance Testing:**
```powershell
It "Should complete in under 5 seconds" {
    $time = Measure-Command {
        Get-ProcessInfo -Top 100
    }
    $time.TotalSeconds | Should -BeLessThan 5
}
```

**Common Performance Issues:**
- ðŸŒ Unnecessary loops
- ðŸŒ Not using -Filter parameters
- ðŸŒ Calling external commands repeatedly
- ðŸŒ Not using pipelines efficiently
- ðŸŒ Building strings in loops

**Optimization Techniques:**
```powershell
# Slow - string concatenation
$result = ""
foreach ($item in $items) {
    $result += "$item`n"
}

# Fast - array join
$result = $items -join "`n"

# Slow - multiple cmdlet calls
foreach ($user in $users) {
    Get-ADUser -Identity $user
}

# Fast - single call with filter
Get-ADUser -Filter "Name -like '*'"
```

**Speaker Notes:**
- Performance matters for production scripts
- Script running on 1000 servers needs to be efficient
- Measure-Command is your friend
- Wrap any code block to time it
- Identify bottlenecks before optimizing
- "Premature optimization is the root of all evil"
- But measuring is not premature
- You can test performance with Pester
- "Function should complete in under X seconds"
- Catches performance regressions
- Common issue: unnecessary loops
- PowerShell has powerful built-in operators
- String concatenation in loops is slow
- Each += creates a new string
- Use -join or StringBuilder instead
- Multiple cmdlet calls are expensive
- Especially with Active Directory
- Use -Filter to get all results at once
- Pipeline abuse can be slow
- Sometimes ForEach-Object is slower than foreach
- Profile before optimizing
- Use different approaches for different scales
- 10 items? Doesn't matter much
- 10,000 items? Big difference
- Test with realistic data sizes
- Performance testing in pipeline
- Track metrics over time
- Alert if execution time spikes
- Good performance = good user experience

---

## Backup Slide 6: Documentation Best Practices
### Making Your Code Understandable

**Comment-Based Help Structure:**
```powershell
function Verb-Noun {
    <#
    .SYNOPSIS
    One-line description
    
    .DESCRIPTION
    Detailed explanation of what the function does
    Multiple paragraphs are fine
    
    .PARAMETER ParameterName
    Describe what this parameter does
    
    .EXAMPLE
    Verb-Noun -ParameterName "value"
    Describe what this example does
    
    .EXAMPLE
    Verb-Noun -Other "example"
    You can have multiple examples
    
    .NOTES
    Author: Your Name
    Date: 2025-01-15
    Version: 1.0
    
    .LINK
    https://docs.example.com/function
    #>
}
```

**Inline Comments:**
```powershell
# Good comment - explains WHY
# Using regex here because wildcard doesn't support OR logic
$pattern = '^(user|admin|guest)'

# Bad comment - explains WHAT (obvious)
# Loop through users
foreach ($user in $users) {
```

**README.md Content:**
1. What does this project do?
2. How do I install/run it?
3. What are the prerequisites?
4. Examples of usage
5. How to contribute
6. License information

**Speaker Notes:**
- Documentation is as important as code
- Future you will thank present you
- Comment-based help is PowerShell standard
- Users can run Get-Help on your function
- Synopsis: One line, answers "what does it do?"
- Description: More detail, use cases
- Parameter: Each one should be explained
- Not just "The name parameter"
- Explain what it's used for, valid values
- Examples are crucial
- Show real usage scenarios
- Copy-paste ready
- Notes section for metadata
- Author, date, version
- Change history can go here
- Links to related documentation
- Inline comments: Explain WHY not WHAT
- Code shows what you're doing
- Comments explain why you chose this approach
- Don't comment obvious things
- "i++" doesn't need a comment
- Complex regex or logic needs explanation
- README.md is first thing people see
- Answer basic questions immediately
- Installation, usage, examples
- Good docs reduce support burden
- Fewer "how do I use this?" questions
- Keep docs updated with code
- Outdated docs are worse than no docs
- Consider using platyPS for external help
- Generates MAML from comment-based help

---

## Backup Slide 7: Troubleshooting Pipeline Failures
### Common Issues and Fixes

**Problem 1: Module Not Found**
```
Error: Module 'PSScriptAnalyzer' not found
```
**Solution:**
```yaml
- task: PowerShell@2
  inputs:
    script: |
      Install-Module PSScriptAnalyzer -Force -Scope CurrentUser
```

**Problem 2: Test Results Not Published**
```
Warning: No test results found
```
**Solution:**
- Check OutputPath in Pester configuration
- Verify PublishTestResults path matches
- Ensure test runs even if some fail

**Problem 3: Permission Denied**
```
Error: Access to path is denied
```
**Solution:**
- Check file permissions
- Don't write to system directories
- Use $(Agent.TempDirectory)

**Problem 4: Timeout**
```
Error: Job timed out after 60 minutes
```
**Solution:**
```yaml
jobs:
- job: TestJob
  timeoutInMinutes: 120
```

**Problem 5: Script Fails Locally But Not in Pipeline**
**Solution:**
- Check PowerShell version differences
- Verify module versions
- Look for environment-specific paths
- Check for dependencies on local files

**Debugging Techniques:**
```powershell
# Add verbose output
Write-Host "##[debug]Variable value: $myVar"

# Enable verbose preference
$VerbosePreference = 'Continue'

# Add step to show environment
Write-Host "PowerShell Version: $($PSVersionTable.PSVersion)"
Write-Host "Current Path: $(Get-Location)"
```

**Speaker Notes:**
- Pipelines will fail - it's normal
- Key is knowing how to troubleshoot
- Module not found is most common
- Happens when running on clean agent
- Solution: Install modules in pipeline
- Force and Scope CurrentUser important
- Test results not publishing
- Usually a path mismatch
- Pester outputs to one path
- PublishTestResults looks in another
- Use consistent variables
- Permission issues happen
- Pipeline runs as service account
- Might not have access to certain folders
- Use pipeline variables for paths
- $(Agent.TempDirectory) is safe
- Timeouts on long-running tests
- Default is 60 minutes
- Increase with timeoutInMinutes
- Or optimize slow tests
- Works locally, fails in pipeline
- Usually environment differences
- Different PowerShell versions
- Different module versions
- Different working directories
- Add debug steps to compare
- Use ##[debug] for Azure DevOps logging
- These show in logs even for passing builds
- Add environment info at start
- PowerShell version, modules, path
- Makes troubleshooting easier
- Enable verbose logging temporarily
- See detailed execution flow
- Each failure is a learning opportunity
- Document solutions in wiki

---

## Backup Slide 8: Integration with Other Tools
### Extending Your Workflow

**VS Code Integration:**
```json
// .vscode/tasks.json
{
    "label": "Run All Tests",
    "type": "shell",
    "command": ".\\src\\controller.ps1 -Action test",
    "group": "test"
}
```
- Run tests with Ctrl+Shift+P â†’ "Run Test Task"
- Pester Test Adapter extension
- PowerShell extension with IntelliSense

**Git Hooks:**
```powershell
# .git/hooks/pre-commit
#!/usr/bin/env pwsh
.\src\controller.ps1 -Action lint
if ($LASTEXITCODE -ne 0) {
    Write-Error "Lint failed. Fix issues before committing."
    exit 1
}
```

**Slack/Teams Notifications:**
```yaml
- task: PowerShell@2
  condition: failed()
  inputs:
    script: |
      $webhook = "$(SlackWebhook)"
      $body = @{
        text = "Build failed: $(Build.BuildNumber)"
      }
      Invoke-RestMethod -Uri $webhook -Method Post -Body ($body | ConvertTo-Json)
```

**Monitoring & Alerting:**
- Application Insights for script telemetry
- Azure Monitor for pipeline health
- Custom dashboards in Azure DevOps
- Email notifications on failures

**Documentation Generation:**
- platyPS for external help files
- PSDoc for markdown documentation
- ReadTheDocs for hosting docs

**Speaker Notes:**
- Your workflow doesn't exist in isolation
- Integration with other tools enhances productivity
- VS Code is the best PowerShell editor
- Built-in terminal, debugging, Git integration
- Tasks.json lets you run commands from UI
- Keyboard shortcuts for common operations
- Pester Test Adapter shows tests in sidebar
- Run individual tests with one click
- Git hooks enforce quality locally
- Pre-commit hook runs lint
- Prevents committing bad code
- Pre-push hook runs tests
- Catches issues before pipeline
- Notifications keep team informed
- Slack or Teams messages on failure
- @mention responsible person
- Include build link for quick access
- Monitoring tracks health over time
- How often do builds fail?
- Which tests fail most?
- How long do builds take?
- Application Insights for production scripts
- Track execution, errors, performance
- Debug issues in production
- Documentation generation
- platyPS creates XML help from comments
- PSDoc generates markdown docs
- ReadTheDocs hosts beautiful documentation
- Integration requires initial setup
- But saves time long-term
- Choose integrations that solve real problems
- Don't integrate for integration's sake

---

## Final Summary Slide
### Your Action Plan

**Today You Learned:**
âœ… How to structure PowerShell projects  
âœ… How to write testable functions  
âœ… How to use PSScriptAnalyzer and Pester  
âœ… How to create Azure DevOps pipelines  
âœ… How to track quality over time  

**This Week:**
1. Install the tools
2. Clone the demo repository
3. Run tests locally
4. Modify a function and see tests fail/pass

**This Month:**
1. Apply this to one of your scripts
2. Set up an Azure DevOps pipeline
3. Share with your team
4. Start building the habit

**Remember:**
- Progress over perfection
- Start small, iterate
- Automation saves time
- Quality is a journey, not a destination

**Thank you and happy scripting!** ðŸš€

**Speaker Notes:**
- We've covered a lot today
- Don't feel overwhelmed
- You don't need to implement everything at once
- Key takeaway: Testing makes code better
- Automation makes testing sustainable
- Start with one small script
- Add one test
- See it pass
- Feel the confidence boost
- Then add another test
- Build the habit gradually
- Share your success with the team
- Teaching others reinforces your learning
- The PowerShell community is supportive
- Don't hesitate to ask for help
- Remember: Every expert was once a beginner
- Every large project started small
- Your journey to better PowerShell starts now
- Thank you for your time and attention
- Feel free to reach out with questions
- Let's build better PowerShell together!
